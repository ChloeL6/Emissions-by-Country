{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "from hashlib import md5\n",
    "from typing import List\n",
    "import uuid\n",
    "from pandas.io.json import json_normalize\n",
    "import pycountry\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "key_path = \"/home/reed/.creds/emissions-team-project.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** SETUP LOGGING ****\n",
    "# setup logging and logger\n",
    "logging.basicConfig(            # setting up the root logger\n",
    "    format='[%(levelname)-5s][%(asctime)s][%(module)s:%(lineno)04d] : %(message)s',\n",
    "    level=logging.INFO,\n",
    "    stream=sys.stdout\n",
    ")\n",
    "logger: logging.Logger = logging.getLogger('root')      # alias the root logger as `logger`\n",
    "logger.setLevel(logging.DEBUG)                          # programmatically reassign the logging level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** BIQUERY SCHEMA TABLE SETUP ****\n",
    "\n",
    "FACTS_TABLE_METADATA = {\n",
    "    'dim_country': {\n",
    "        'table_name': 'dim_country',\n",
    "        'schema': [\n",
    "            # indexes are written if only named in the schema\n",
    "            bigquery.SchemaField('country_code', 'string', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('country_code_2', 'string', mode='NULLABLE'),\n",
    "            bigquery.SchemaField('country', 'string', mode='NULLABLE'),\n",
    "            bigquery.SchemaField('land_area_square_km', 'float64', mode='NULLABLE'),\n",
    "            bigquery.SchemaField('latitude', 'float64', mode='NULLABLE'),\n",
    "            bigquery.SchemaField('longitude', 'float64', mode='NULLABLE'),\n",
    "            bigquery.SchemaField('created_at', 'timestamp', mode='NULLABLE'),\n",
    "            bigquery.SchemaField('modified_at', 'timestamp', mode='NULLABLE'),\n",
    "        ]\n",
    "    }      \n",
    "}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG][2023-01-18 16:40:48,601][retry:0351] : Converted retries value: 3 -> Retry(total=3, connect=None, read=None, redirect=None, status=None)\n",
      "[DEBUG][2023-01-18 16:40:48,627][requests:0192] : Making request: POST https://oauth2.googleapis.com/token\n",
      "[DEBUG][2023-01-18 16:40:48,628][connectionpool:1003] : Starting new HTTPS connection (1): oauth2.googleapis.com:443\n",
      "[DEBUG][2023-01-18 16:40:48,812][connectionpool:0456] : https://oauth2.googleapis.com:443 \"POST /token HTTP/1.1\" 200 None\n",
      "[DEBUG][2023-01-18 16:40:48,814][connectionpool:1003] : Starting new HTTPS connection (1): bigquery.googleapis.com:443\n",
      "[DEBUG][2023-01-18 16:40:49,111][connectionpool:0456] : https://bigquery.googleapis.com:443 \"POST /bigquery/v2/projects/emissions-team-project/datasets?prettyPrint=false HTTP/1.1\" 409 None\n",
      "[DEBUG][2023-01-18 16:40:49,360][connectionpool:0456] : https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/emissions-team-project/datasets/emissions?prettyPrint=false HTTP/1.1\" 200 None\n",
      "[INFO ][2023-01-18 16:40:49,362][717801439:0012] : Created emissions dataset: emissions-team-project:emissions\n"
     ]
    }
   ],
   "source": [
    "# change to match your filesystem\n",
    "PROJECT_NAME = \"emissions-team-project\"\n",
    "DATASET_NAME = \"emissions\"\n",
    "\n",
    "# **** BIGQUERY DATASET CREATION ****\n",
    "\n",
    "dataset_id = f\"{PROJECT_NAME}.{DATASET_NAME}\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = \"US\"\n",
    "dataset = client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "logger.info(f\"Created emissions dataset: {dataset.full_dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dim_country\n",
    "df = pd.read_csv('./data/Emissions_by_Country_2002-2022.csv')\n",
    "lat_long_df = pd.read_csv('./data/world_country_and_usa_states_latitude_and_longitude_values.csv')\n",
    "df1 = df[['Country', 'ISO 3166-1 alpha-3']]\n",
    "#make a 2 letter country code\n",
    "def alpha3code(column):\n",
    "    CODE=[]\n",
    "    for country in column:\n",
    "        try:\n",
    "            code=pycountry.countries.get(name=country).alpha_3\n",
    "           # .alpha_3 means 3-letter country code \n",
    "           # .alpha_2 means 2-letter country code\n",
    "            CODE.append(code)\n",
    "        except:\n",
    "            CODE.append('None')\n",
    "    return CODE\n",
    "    \n",
    "#make a 3 digit country code column in my lat_long_df and fix the ones that the function missed\n",
    "lat_long_df['long_code']=alpha3code(lat_long_df.country)\n",
    "lat_long_df = lat_long_df[['country','long_code','country_code','latitude','longitude']]\n",
    "lat_long_df = lat_long_df.rename(columns={'country_code': 'country_code_2', 'long_code': 'country_code'})\n",
    "lat_long_df.at[26,'country_code']='BRN'\n",
    "lat_long_df.at[27,'country_code']='BOL'\n",
    "lat_long_df.at[36,'country_code']='CCK'\n",
    "lat_long_df.at[37,'country_code']='COD'\n",
    "lat_long_df.at[39,'country_code']='COG'\n",
    "lat_long_df.at[49,'country_code']='CPV'\n",
    "lat_long_df.at[52,'country_code']='CZE'\n",
    "lat_long_df.at[68,'country_code']='FLK'\n",
    "lat_long_df.at[69,'country_code']='FSM'\n",
    "lat_long_df.at[105,'country_code']='IRN'\n",
    "lat_long_df.at[118,'country_code']='PRK'\n",
    "lat_long_df.at[119,'country_code']='KOR'\n",
    "lat_long_df.at[123,'country_code']='LAO'\n",
    "lat_long_df.at[136,'country_code']='MDA'\n",
    "lat_long_df.at[140,'country_code']='MKD'\n",
    "lat_long_df.at[142,'country_code']='MMR'\n",
    "lat_long_df.at[144,'country_code']='MAC'\n",
    "lat_long_df.at[177,'country_code']='PCN'\n",
    "lat_long_df.at[179,'country_code']='PSE'\n",
    "lat_long_df.at[187,'country_code']='RUS'\n",
    "lat_long_df.at[195,'country_code']='SHN'\n",
    "lat_long_df.at[204,'country_code']='STP'\n",
    "lat_long_df.at[206,'country_code']='SYR'\n",
    "lat_long_df.at[207,'country_code']='SWZ'\n",
    "lat_long_df.at[222,'country_code']='TWN'\n",
    "lat_long_df.at[223,'country_code']='TZA'\n",
    "lat_long_df.at[226,'country_code']='UMI'\n",
    "lat_long_df.at[230,'country_code']='VAT'\n",
    "lat_long_df.at[232,'country_code']='VEN'\n",
    "lat_long_df.at[233,'country_code']='VGB'\n",
    "lat_long_df.at[234,'country_code']='VIR'\n",
    "lat_long_df.at[235,'country_code']='VNM'\n",
    "lat_long_df.at[234,'country_code']='VIR'\n",
    "\n",
    "#read country size csv\n",
    "size_df = pd.read_csv('./data/country_size.csv', dtype=str)\n",
    "size_df = size_df[['Country Code','2020']]\n",
    "size_df[\"2020\"] = size_df['2020'].astype('float').round(decimals=0)\n",
    "size_df = size_df.rename(columns={'2020': 'land_area_square_km', 'Country Code': 'country_code'})\n",
    "\n",
    "#merge datasets to create dim_countries\n",
    "dim_country = pd.merge(lat_long_df,size_df, how='left', on='country_code')\n",
    "dim_country = dim_country[['country', 'country_code','country_code_2','land_area_square_km','latitude','longitude']]\n",
    "dim_country[\"created_at\"] = pd.Timestamp.now()\n",
    "dim_country[\"modified_at\"] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the load table function\n",
    "\n",
    "def load_table(\n",
    "    df: pd.DataFrame, \n",
    "    client: bigquery.Client, \n",
    "    table_name: str, \n",
    "    schema: List[bigquery.SchemaField], \n",
    "    create_disposition: str = 'CREATE_IF_NEEDED', \n",
    "    write_disposition: str = 'WRITE_TRUNCATE'\n",
    "    ) -> None:\n",
    "    \"\"\"load dataframe into bigquery table\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe to load\n",
    "        client (bigquery.Client): bigquery client\n",
    "        table_name (str): full table name including project and dataset id\n",
    "        schema (List[bigquery.SchemaField]): table schema with data types\n",
    "        create_disposition (str, optional): create table disposition. Defaults to 'CREATE_IF_NEEDED'.\n",
    "        write_disposition (str, optional): overwrite table disposition. Defaults to 'WRITE_TRUNCATE'.\n",
    "    \"\"\"\n",
    "    # *** run some checks ***\n",
    "    # test table name to be full table name including project and dataset name. It must contain to dots\n",
    "    assert len(table_name.split('.')) == 3, f\"Table name must be a full bigquery table name including project and dataset id: '{table_name}'\"\n",
    "    # setup bigquery load job:\n",
    "    #  create table if needed, replace rows, define the table schema\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        create_disposition=create_disposition,\n",
    "        write_disposition=write_disposition,\n",
    "        schema=schema\n",
    "    )\n",
    "    logger.info(f\"loading table: '{table_name}'\")\n",
    "    job = client.load_table_from_dataframe(df, destination=table_name, job_config=job_config)\n",
    "    job.result()        # wait for the job to finish\n",
    "    # get the resulting table\n",
    "    table = client.get_table(table_name)\n",
    "    logger.info(f\"loaded {table.num_rows} rows into {table.full_table_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO ][2023-01-18 16:40:49,528][2444475212:0031] : loading table: 'emissions-team-project.emissions.dim_country'\n",
      "[DEBUG][2023-01-18 16:40:50,648][connectionpool:0456] : https://bigquery.googleapis.com:443 \"POST /upload/bigquery/v2/projects/emissions-team-project/jobs?uploadType=multipart HTTP/1.1\" 200 2188\n",
      "[DEBUG][2023-01-18 16:40:50,794][connectionpool:0456] : https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/emissions-team-project/jobs/96e12b19-c94b-45e2-9926-94b6dff3fe9e?location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
      "[DEBUG][2023-01-18 16:40:50,795][retry:0214] : Retrying due to , sleeping 0.2s ...\n",
      "[DEBUG][2023-01-18 16:40:51,092][connectionpool:0456] : https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/emissions-team-project/jobs/96e12b19-c94b-45e2-9926-94b6dff3fe9e?location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
      "[DEBUG][2023-01-18 16:40:51,093][retry:0214] : Retrying due to , sleeping 1.2s ...\n",
      "[DEBUG][2023-01-18 16:40:52,462][connectionpool:0456] : https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/emissions-team-project/jobs/96e12b19-c94b-45e2-9926-94b6dff3fe9e?location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
      "[DEBUG][2023-01-18 16:40:52,464][retry:0214] : Retrying due to , sleeping 0.7s ...\n",
      "[DEBUG][2023-01-18 16:40:53,304][connectionpool:0456] : https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/emissions-team-project/jobs/96e12b19-c94b-45e2-9926-94b6dff3fe9e?location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
      "[DEBUG][2023-01-18 16:40:53,478][connectionpool:0456] : https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/emissions-team-project/datasets/emissions/tables/dim_country?prettyPrint=false HTTP/1.1\" 200 None\n",
      "[INFO ][2023-01-18 16:40:53,480][2444475212:0036] : loaded 245 rows into emissions-team-project:emissions.dim_country\n",
      "[INFO ][2023-01-18 16:40:53,480][1653439129:0009] : loaded dim_country facts\n"
     ]
    }
   ],
   "source": [
    "# Load dim_country dimension table\n",
    "\n",
    "# get table name and schema from FACTS_TABLE_METADATA config param\n",
    "table_name = f\"{PROJECT_NAME}.{DATASET_NAME}.{FACTS_TABLE_METADATA['dim_country']['table_name']}\"\n",
    "schema = FACTS_TABLE_METADATA['dim_country']['schema']\n",
    "# load dataframe\n",
    "load_table(dim_country, client, table_name, schema)\n",
    "\n",
    "logger.info(f\"loaded dim_country facts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2df45134b32dd0c23fe121b1bef65c867c7b1238f7afdcb636246ba4c55b1ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
